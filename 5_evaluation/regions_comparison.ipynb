{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Region comparison\n",
    "Compare different regions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Preparation\n",
    "- Imports\n",
    "- Declare classifiers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-cf938bbc4b8f>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mneighbors\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mKNeighborsClassifier\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensemble\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mRandomForestClassifier\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mxgboost\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mXGBClassifier\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mpreprocessing\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel_selection\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 7)\n",
    "\n",
    "classifiers = []\n",
    "\n",
    "classifiers.append(['Decision Tree', DecisionTreeClassifier()])\n",
    "classifiers.append(['K Nearest Neighbor', KNeighborsClassifier(n_jobs=-1)])\n",
    "classifiers.append(['Random Forest', RandomForestClassifier(n_jobs=-1)])\n",
    "classifiers.append(['XG Boost', XGBClassifier(use_label_encoder=False, verbosity=0)])\n",
    "classifiers.append(['SVM', SVC(kernel='rbf', random_state = 1)])\n",
    "\n",
    "classifiers.append(['Decision Tree (optimized)', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None, max_features=None, max_leaf_nodes=50, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, splitter='best')])\n",
    "classifiers.append(['K Nearest Neighbor (optimized)', KNeighborsClassifier(n_jobs=-1, algorithm='kd_tree', leaf_size=20, n_neighbors=8, p=1, weights='uniform')])\n",
    "classifiers.append(['Random Forest (optimized)', RandomForestClassifier(n_jobs=-1, class_weight=None, criterion='gini', max_depth=9, max_features='log2', max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=4, min_weight_fraction_leaf=0.0, n_estimators=200)])\n",
    "classifiers.append(['XG Boost (optimized)', XGBClassifier(use_label_encoder=False, verbosity=0, booster='gbtree', colsample_bylevel=0.75, colsample_bynode=1, colsample_bytree=1, gamma=2, learning_rate=0.5, max_delta_step=0, max_depth=6, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=0, subsample=1, tree_method='hist')])\n",
    "\n",
    "classifiers.append(['K Nearest Neighbor (maxed)', KNeighborsClassifier(n_jobs=-1, n_neighbors=300)])"
   ]
  },
  {
   "source": [
    "## Iterate over regions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "regions = ['DE', 'GB', 'US']\n",
    "results = pd.DataFrame(columns=['region', 'algorithm', 'accuracy', 'execution_time'])\n",
    "\n",
    "for region in regions:\n",
    "    # Data preperation\n",
    "    df = pd.read_csv('../0_data/' + region + 'videos.csv',\n",
    "    low_memory=False)\n",
    "\n",
    "    df['trending_date'] = df.apply(lambda row: datetime.strptime(row['trending_date'], '%y.%d.%m'), axis=1)\n",
    "    df['publish_time'] = df.apply(lambda row: datetime.strptime(row['publish_time'], '%Y-%m-%dT%H:%M:%S.000Z'), axis=1)\n",
    "    df['days_until_trending'] = df.apply(lambda row: ((row['trending_date'] - row['publish_time']).days + 1), axis=1)\n",
    "\n",
    "    df['tags_count'] = df.apply(lambda row: len(row['tags'].split('|')), axis=1)\n",
    "    df['publish_hour'] = df['publish_time'].map(lambda x: x.hour)\n",
    "    df['publish_month'] = df['publish_time'].map(lambda x: x.month)\n",
    "    df['publish_year'] = df['publish_time'].map(lambda x: x.year)\n",
    "    df['publish_day_of_month'] = df['publish_time'].map(lambda x: x.day)\n",
    "    df['publish_weekday'] = df['publish_time'].map(lambda x: x.weekday()) # 0: Monday, 6: Sunday\n",
    "\n",
    "    df['like_dislike_ratio'] = df.apply(lambda row: row['likes'] / (row['dislikes'] + 1), axis=1)\n",
    "    df['like_view_ratio'] = df.apply(lambda row: row['likes'] / (row['views'] + 1), axis=1)\n",
    "\n",
    "    df['ratings'] = df['likes'] + df['dislikes']\n",
    "    df['likes_per_rating'] = df.apply(lambda row: 0 if row['ratings'] == 0 else row['likes'] / row['ratings'], axis=1)\n",
    "    df['ratings_per_view'] = df['ratings'] / df['views']\n",
    "    df['comments_per_view'] = df['comment_count'] / df['views']\n",
    "\n",
    "    def assign_target_category(row):\n",
    "        if row['days_until_trending'] == 0: \n",
    "            return 0\n",
    "        elif row['days_until_trending'] == 1:\n",
    "            return 1\n",
    "        elif row['days_until_trending'] == 2:\n",
    "            return 2\n",
    "        elif row['days_until_trending'] <= 5:\n",
    "            return 3\n",
    "        else:\n",
    "            return 6\n",
    "\n",
    "    df['target_category'] = df.apply(assign_target_category, axis=1)\n",
    "    df['channel_title'] = df['channel_title'].astype('category')\n",
    "\n",
    "    tag_df = pd.read_csv('../0_data/' + region + 'tags.csv')\n",
    "    tag_df = tag_df.set_index('tag')\n",
    "    def calculate_tag_factor(tag_string, tag_data):\n",
    "        tag_list = pd.Series(list(set(map(lambda x: x.strip('\\\"').lower(), tag_string.split('|')))))\n",
    "        return tag_list.apply(lambda tag: tag_data['factor'].get(tag, np.nan)).mean(skipna=True)\n",
    "        \n",
    "    df['tag_factors'] = df['tags'].apply(lambda x: calculate_tag_factor(x, tag_df))\n",
    "    df['tag_factors'] = df.apply(lambda row: 0 if np.isnan(row['tag_factors']) else row['tag_factors'], axis=1)\n",
    "\n",
    "    N = len(df)\n",
    "    dropColumns = ['video_id', 'title', 'tags', 'thumbnail_link', 'description']\n",
    "    for column in df.columns:\n",
    "        numberOfUniqueValues = df[column].nunique()\n",
    "        if numberOfUniqueValues < 2:\n",
    "            dropColumns.append(column)\n",
    "        elif df[column].dtype == 'object' and numberOfUniqueValues > N * 0.9:\n",
    "            dropColumns.append(column)\n",
    "        elif df[column].isna().sum() / N > 0.95:\n",
    "            dropColumns.append(column)\n",
    "            \n",
    "    df.drop(columns=dropColumns, inplace=True)\n",
    "\n",
    "    # Encode features\n",
    "    x_df = DataFrame(index=df.index)\n",
    "    features = ['views', 'publish_hour', 'ratings_per_view', 'comments_per_view', 'tag_factors']\n",
    "    for feature in features:\n",
    "        feature_data = df[feature]\n",
    "        if df[feature].dtype.name == 'category':\n",
    "            x_label_encoder = preprocessing.LabelEncoder()\n",
    "            x_label_encoder.fit(feature_data.astype(str))\n",
    "            x_df[feature] = x_label_encoder.transform(feature_data)\n",
    "        elif df[feature].dtype.name == 'datetime64[ns]':\n",
    "            x_df[feature] = feature_data.to_seconds()\n",
    "        elif df[feature].dtype.name == 'bool':\n",
    "            x_df[feature] = int(feature_data)\n",
    "        else:\n",
    "            x_df[feature] = feature_data\n",
    "\n",
    "    x = np.reshape(x_df, (-1, len(x_df.columns)))\n",
    "\n",
    "    # Encode target\n",
    "    target = df['target_category'].astype(str)\n",
    "    y_label_encoder = preprocessing.LabelEncoder()\n",
    "    y_label_encoder.fit(target)\n",
    "    y = y_label_encoder.transform(target)\n",
    "\n",
    "    # Test with multiple train-test splits\n",
    "    for i in range(5):\n",
    "\n",
    "        # Create data subsets\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.40, random_state=i)\n",
    "\n",
    "        # Train model and predict test data\n",
    "        for classifier in classifiers:\n",
    "            start_time = time.time()\n",
    "            classifier[1].fit(x_train, y_train)\n",
    "            y_pred = classifier[1].predict(x_test)\n",
    "            execution_time = time.time() - start_time\n",
    "            result = {'region': region, 'algorithm': classifier[0], 'accuracy': accuracy_score(y_test, y_pred), 'execution_time': execution_time}\n",
    "            results = results.append(result, ignore_index=True)"
   ]
  },
  {
   "source": [
    "## Results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head(15)"
   ]
  },
  {
   "source": [
    "Accuracy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=results, x='region', y='accuracy', hue='algorithm')"
   ]
  },
  {
   "source": [
    "Execution time"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=results, x='region', y='execution_time', hue='algorithm')"
   ]
  }
 ]
}