{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "341fe68f31b2607f2567ea4631bf318bef91dbbea98de31e2266395fcf799846"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Predicion model optimization\n",
    "\n",
    "## Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "source": [
    "## Data preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/DEvideos.csv',\n",
    "    low_memory=False)\n",
    "\n",
    "df['trending_date'] = df.apply(lambda row: datetime.strptime(row['trending_date'], '%y.%d.%m'), axis=1)\n",
    "df['publish_time'] = df.apply(lambda row: datetime.strptime(row['publish_time'], '%Y-%m-%dT%H:%M:%S.000Z'), axis=1)\n",
    "df['days_until_trending'] = df.apply(lambda row: ((row['trending_date'] - row['publish_time']).days + 1), axis=1)\n",
    "\n",
    "df['tags_count'] = df.apply(lambda row: len(row['tags'].split('|')), axis=1)\n",
    "df['publish_hour'] = df['publish_time'].map(lambda x: x.hour)\n",
    "df['publish_month'] = df['publish_time'].map(lambda x: x.month)\n",
    "df['publish_year'] = df['publish_time'].map(lambda x: x.year)\n",
    "df['publish_day_of_month'] = df['publish_time'].map(lambda x: x.day)\n",
    "df['publish_weekday'] = df['publish_time'].map(lambda x: x.weekday()) # 0: Monday, 6: Sunday\n",
    "\n",
    "df['like_dislike_ratio'] = df.apply(lambda row: row['likes'] / (row['dislikes'] + 1), axis=1)\n",
    "df['like_view_ratio'] = df.apply(lambda row: row['likes'] / (row['views'] + 1), axis=1)\n",
    "\n",
    "df['ratings'] = df['likes'] + df['dislikes']\n",
    "df['likes_per_rating'] = df.apply(lambda row: 0 if row['ratings'] == 0 else row['likes'] / row['ratings'], axis=1)\n",
    "df['ratings_per_view'] = df['ratings'] / df['views']\n",
    "df['comments_per_view'] = df['comment_count'] / df['views']\n",
    "\n",
    "# Using int instead of cat\n",
    "def assign_target_category(row):\n",
    "    if row['days_until_trending'] == 0: \n",
    "        return 0\n",
    "    elif row['days_until_trending'] == 1:\n",
    "        return 1\n",
    "    elif row['days_until_trending'] == 2:\n",
    "        return 2\n",
    "    elif row['days_until_trending'] <= 5:\n",
    "        return 3\n",
    "    else:\n",
    "        return 6\n",
    "\n",
    "df['target_category'] = df.apply(assign_target_category, axis=1)\n",
    "df['channel_title'] = df['channel_title'].astype('category')"
   ]
  },
  {
   "source": [
    "### Map tag factor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_df = pd.read_csv('../data/tags.csv')\n",
    "tag_df = tag_df.set_index('tag')\n",
    "def calculate_tag_factor(tag_string, tag_data):\n",
    "    tag_list = pd.Series(list(set(map(lambda x: x.strip('\\\"').lower(), tag_string.split('|')))))\n",
    "    return tag_list.apply(lambda tag: tag_data['factor'].get(tag, np.nan)).mean(skipna=True)\n",
    "    \n",
    "df['tag_factors'] = df['tags'].apply(lambda x: calculate_tag_factor(x, tag_df))\n",
    "df['tag_factors'] = df.apply(lambda row: 0 if np.isnan(row['tag_factors']) else row['tag_factors'], axis=1)"
   ]
  },
  {
   "source": [
    "### Remove unneccessary columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df)\n",
    "dropColumns = ['video_id', 'title', 'tags', 'thumbnail_link', 'description']\n",
    "for column in df.columns:\n",
    "    numberOfUniqueValues = df[column].nunique()\n",
    "    if numberOfUniqueValues < 2:\n",
    "        dropColumns.append(column)\n",
    "    elif df[column].dtype == 'object' and numberOfUniqueValues > N * 0.9:\n",
    "        dropColumns.append(column)\n",
    "    elif df[column].isna().sum() / N > 0.95:\n",
    "        dropColumns.append(column)\n",
    "        \n",
    "df.drop(columns=dropColumns, inplace=True)"
   ]
  },
  {
   "source": [
    "## Encode features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = DataFrame(index=df.index)\n",
    "features = ['views', 'publish_hour', 'tag_factors']\n",
    "for feature in features:\n",
    "    feature_data = df[feature]\n",
    "    if df[feature].dtype.name == 'category':\n",
    "        x_label_encoder = preprocessing.LabelEncoder()\n",
    "        x_label_encoder.fit(feature_data.astype(str))\n",
    "        x_df[feature] = x_label_encoder.transform(feature_data)\n",
    "    elif df[feature].dtype.name == 'datetime64[ns]':\n",
    "        x_df[feature] = feature_data.to_seconds()\n",
    "    elif df[feature].dtype.name == 'bool':\n",
    "        x_df[feature] = int(feature_data)\n",
    "    else:\n",
    "        x_df[feature] = feature_data\n",
    "\n",
    "x = np.reshape(x_df, (-1, len(x_df.columns)))"
   ]
  },
  {
   "source": [
    "## Encode prediction target"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['target_category'].astype(str)\n",
    "y_label_encoder = preprocessing.LabelEncoder()\n",
    "y_label_encoder.fit(target)\n",
    "y = y_label_encoder.transform(target)"
   ]
  },
  {
   "source": [
    "## Calculate best prediction parameters\n",
    "\n",
    "### XGBoost\n",
    "estimated >24h\n",
    "\n",
    "*Pratally executed*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6656953966699314\n{'booster': 'gbtree', 'colsample_bylevel': 0.75, 'colsample_bynode': 1, 'colsample_bytree': 1, 'gamma': 2, 'learning_rate': 0.5, 'max_delta_step': 0, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 100, 'reg_alpha': 1, 'reg_lambda': 0, 'subsample': 1, 'tree_method': 'hist'}\nWall time:       \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, verbosity=0)\n",
    "optimization_dict = {'n_estimators': [10, 50, 100, 200],\n",
    "                     'max_depth': [3, 6, 9],\n",
    "                     'learning_rate': [0.1, 0.3, 0.5],\n",
    "                     'booster': ['gbtree', 'gblinear', 'dart'],\n",
    "                     'tree_method': ['exact', 'approx', 'hist'],\n",
    "                     'gamma': [0, 2, 4],\n",
    "                     'min_child_weight': [1, 2, 4],\n",
    "                     'max_delta_step': [0, 2, 4, 8],\n",
    "                     'subsample': [0.5, 0.75, 1],\n",
    "                     'colsample_bytree': [0.5, 0.75, 1],\n",
    "                     'colsample_bylevel': [0.5, 0.75, 1],\n",
    "                     'colsample_bynode': [0.5, 0.75, 1],\n",
    "                     'reg_alpha': [0, 1, 2],\n",
    "                     'reg_lambda': [0, 1, 2]}\n",
    "\n",
    "model = GridSearchCV(xgb_model, optimization_dict, scoring='accuracy')\n",
    "\n",
    "model.fit(x,y)\n",
    "print(model.best_score_)\n",
    "print(model.best_params_)"
   ]
  },
  {
   "source": [
    "### K Nearest Neighbors\n",
    "~5min"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6014691478942213\n{'algorithm': 'kd_tree', 'leaf_size': 20, 'n_neighbors': 8, 'p': 1, 'weights': 'uniform'}\nWall time: 5min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "knn_model = KNeighborsClassifier()\n",
    "optimization_dict = {'n_neighbors': [2, 5, 8],\n",
    "                    'weights': ['uniform', 'distance'],\n",
    "                    'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "                    'leaf_size': [20, 30, 40],\n",
    "                    'p': [1, 2, 4]}\n",
    "\n",
    "model = GridSearchCV(knn_model, optimization_dict, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "model.fit(x,y)\n",
    "print(model.best_score_)\n",
    "print(model.best_params_)"
   ]
  },
  {
   "source": [
    "### Random Forest\n",
    "~1.5h"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6588393731635651\n{'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 'log2', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 4, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 200}\nWall time: 1h 37min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf_model = RandomForestClassifier()\n",
    "optimization_dict = {'n_estimators': [10, 50, 100, 200],\n",
    "'criterion': ['gini', 'entropy'],\n",
    "'max_depth': [None, 3, 6, 9],\n",
    "'min_samples_split': [1, 2, 4],\n",
    "'min_samples_leaf': [1, 2],\n",
    "'min_weight_fraction_leaf': [0.0, 0.2],\n",
    "'max_features': [None, 'sqrt', 'log2'],\n",
    "'max_leaf_nodes': [None, 50],\n",
    "'min_impurity_decrease': [0.0, 0.2],\n",
    "'class_weight': [None, 'balanced', 'balanced_subsample']}\n",
    "\n",
    "model = GridSearchCV(rf_model, optimization_dict, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "model.fit(x,y)\n",
    "print(model.best_score_)\n",
    "print(model.best_params_)"
   ]
  },
  {
   "source": [
    "### Decision tree\n",
    "~30sek"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6481880509304603\n{'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': 50, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'splitter': 'best'}\nWall time: 33.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf_model = DecisionTreeClassifier()\n",
    "optimization_dict = {'criterion': ['gini', 'entropy'],\n",
    "'splitter': ['best', 'random'],\n",
    "'max_depth': [None, 3, 6, 9],\n",
    "'min_samples_split': [1, 2, 4],\n",
    "'min_samples_leaf': [1, 2],\n",
    "'min_weight_fraction_leaf': [0.0, 0.2],\n",
    "'max_features': [None, 'sqrt', 'log2'],\n",
    "'max_leaf_nodes': [None, 50],\n",
    "'min_impurity_decrease': [0.0, 0.2],\n",
    "'class_weight': [None, 'balanced']}\n",
    "\n",
    "model = GridSearchCV(rf_model, optimization_dict, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "model.fit(x,y)\n",
    "print(model.best_score_)\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}